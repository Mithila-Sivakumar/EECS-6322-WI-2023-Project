{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "A6kLwdGrX8jk",
        "UQuovx25YBka",
        "7gSTzhGCh6Jy",
        "aZsXby_2v5dL",
        "7K8-qN5plWPs",
        "mv-wvbVyk6LJ",
        "n9aTuDv7S8lh",
        "gXzAskSePFUO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Imports**"
      ],
      "metadata": {
        "id": "A6kLwdGrX8jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installs\n",
        "!pip install torch\n",
        "!pip install argparse"
      ],
      "metadata": {
        "id": "XYXI1nHLGorR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting google drive to load the datasets and the reid folders present in QAConv\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/MyDrive/NN/QAConv # If reproducing, change this to your own path"
      ],
      "metadata": {
        "id": "tb3gVE_8zr2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import copy\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import os.path as osp\n",
        "import sys\n",
        "import string\n",
        "import time\n",
        "import json\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.nn.modules import Module\n",
        "from torch.nn.modules.container import ModuleList\n",
        "from torch import einsum\n",
        "from google.colab import drive\n",
        "from __future__ import absolute_import\n",
        "from typing import Optional, Any\n",
        "from torch.nn import Module, ModuleList\n",
        "import torchvision\n",
        "from torch.nn.modules import TransformerEncoderLayer\n",
        "from __future__ import print_function, absolute_import\n",
        "from torch.backends import cudnn\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from reid import datasets\n",
        "from reid.trainers import Trainer\n",
        "from reid.evaluators import Evaluator\n",
        "from reid.utils.data import transforms as T\n",
        "from reid.utils.data.preprocessor import Preprocessor\n",
        "from reid.utils.logging import Logger\n",
        "from reid.utils.serialization import load_checkpoint, save_checkpoint\n",
        "from reid.utils.data.graph_sampler import GraphSampler\n",
        "from reid.loss.pairwise_matching_loss import PairwiseMatchingLoss"
      ],
      "metadata": {
        "id": "5IdGR2PXX_Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**TransformerDecoderLayer**\n",
        "source : https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerDecoder"
      ],
      "metadata": {
        "id": "UQuovx25YBka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(Module):\n",
        "   __constants__ = ['batch_first', 'norm_first']\n",
        "   # using default values for d_model (512) and dim_feedforward (2048) as mentioned in the paper\n",
        "   # def __init__(self, seq_len, , d_model: int = 512, dim_feedforward: int = 2048):\n",
        "   def __init__(self, seq_len, d_model, dim_feedforward):\n",
        "    super(TransformerDecoderLayer, self).__init__()\n",
        "     # parameters\n",
        "    self.seq_len = seq_len  # seq_len = hw\n",
        "    self.d_model = d_model  # this is number of channels, will be 512\n",
        "\n",
        "    # The prior score embeddings are learnable parameters of size hw Ã— hw. They can also be considered\n",
        "    # learnable weights, somewhat similar to the learnable FC weights\n",
        "   \n",
        "    prior_score_weight = torch.randn(self.seq_len, self.seq_len) #sixe hw x hw as mentioned in the paper\n",
        "    # creating prior score as a learnable parameter\n",
        "    self.learnable_prior_score_weight = nn.Parameter(prior_score_weight)  # prior store weights are added to the list of learnable parameters\n",
        "\n",
        "    # Instantiating all the layer according to figure 1 in the paper\n",
        "    self.fc1 = torch.nn.Linear(d_model, d_model, bias=True)     \n",
        "    self.bn1 = torch.nn.BatchNorm1d(2*self.seq_len)   \n",
        "    # 2 MLP heads exactly as explained in the paper  \n",
        "    self.mlphead1 = torch.nn.Sequential(\n",
        "                              torch.nn.Linear(2*self.seq_len, dim_feedforward),\n",
        "                              torch.nn.BatchNorm1d(dim_feedforward),\n",
        "                              torch.nn.ReLU()\n",
        "                              )\n",
        "    self.mlphead2 = torch.nn.Sequential(\n",
        "                                         torch.nn.Linear(dim_feedforward, 1),\n",
        "                                         torch.nn.BatchNorm1d(1))\n",
        "\n",
        "   def forward(self, tgt: Tensor, memory: Tensor, prev_score: Tensor) -> Tensor:\n",
        "    \n",
        "    # tgt and memory are the output of the corresponding parallel encoder's query and gallery encodings\n",
        "    # getting the value of parameters q k h w d as explained in the paper q,k - batch size, h - height, w-width and d- dimension\n",
        "    q, h, w, d = tgt.size()\n",
        "    k, h, w, d = memory.size()\n",
        "   \n",
        "\n",
        "    # Reshapig to prepare tgt and memory for matrix multioplication change tgt from q,h,w,d to q, h*w, d\n",
        "    # ie changing q,h,w,d to q,t,d and k h w d to k s t\n",
        "    tgt = tgt.view(q,-1,d)\n",
        "    memory = memory.view(k,-1,d)\n",
        "  \n",
        "\n",
        "    # passing tgt and memory through the fully connected layer to get query and gallery as explained in the paper\n",
        "    query = self.fc1(tgt)\n",
        "    gallery = self.fc1(memory)\n",
        "\n",
        "    # dot product (batched matrix multiplication) of query and gallery - taken from QA conv as explained in the paper\n",
        "    mat_mul = einsum('q t d, k s d -> q k s t', query, gallery)\n",
        "    #mat_mul = torch.matmul(query, gallery.t())\n",
        "   \n",
        "\n",
        "    # sigmoid of prior-score embedding\n",
        "    prior_score_sig = self.learnable_prior_score_weight.sigmoid()\n",
        "\n",
        "    # element wise multiplication of dot product and output of sigmoid\n",
        "    final_score = mat_mul * prior_score_sig\n",
        "\n",
        "    # Reshape (q,k,s,t) to (q*k, hw,hw) - as explained in the paper\n",
        "    final_score = final_score.reshape(q*k, self.seq_len, self.seq_len)\n",
        "\n",
        "    # GMP layer as it is from the QA conv as explained in the paper\n",
        "    final_score = torch.cat((final_score.max(dim=1)[0], final_score.max(dim=2)[0]), dim=-1)\n",
        "     \n",
        "    #final_score = final_score.max(dim=-1)[0]         # our GMP implementation\n",
        "    \n",
        "    # Pass the score through each layer \n",
        "    final_score = self.bn1(final_score)\n",
        "    final_score = self.mlphead1(final_score)\n",
        "    final_score = self.mlphead2(final_score)\n",
        "    final_score = final_score.view(q, k) # changing the dimesion from scalar to q,k because the loss functiond defined in QAConv requires this\n",
        "    final_score += prev_score\n",
        "    return final_score\n",
        "    "
      ],
      "metadata": {
        "id": "l7j3vZLqYH9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Transmatcher class**\n",
        "\n",
        "Similar to Transformer class implementation of pytorch\n",
        "- Has encoder from pytorch (Transformer Encoder Layer)\n",
        "- Our custom Decoder (Transformer Decoder Layer)"
      ],
      "metadata": {
        "id": "7gSTzhGCh6Jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransMatcher(Module):\n",
        "    def __init__(self, seq_len, d_model: int = 512, dim_feedforward: int = 2048):\n",
        "        super(TransMatcher, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        nhead = 1 # Only 1 head in encoder as defined in paper\n",
        "        dropout = 0.\n",
        "        # The paper used N-1 encoders and N decoders where the output of resnet50 is directly fed to the first decoder. N = 3 from paper\n",
        "        self.encoder_layer1 = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout) #encoder layer 1\n",
        "        self.encoder_layer2 = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout) #encoder layer 2\n",
        "        self.decoder_layer1 = TransformerDecoderLayer(self.seq_len, d_model, dim_feedforward) #decoder layer 1\n",
        "        self.decoder_layer2 = TransformerDecoderLayer(self.seq_len, d_model, dim_feedforward) #decoder layer 2\n",
        "        self.decoder_layer3 = TransformerDecoderLayer(self.seq_len, d_model, dim_feedforward) #decoder layer 3\n",
        "        self.reset_parameters() # from original implementation of transformers\n",
        "\n",
        "    #present in original implementation\n",
        "    def reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                xavier_uniform_(p)\n",
        "    \n",
        "    def forward(self, query_en: Tensor, gallery_en: Tensor) -> Tensor:\n",
        "       # q - length of query\n",
        "       # k - length of gallery\n",
        "       # h, w - size of feature map\n",
        "       # d = channels\n",
        "       print(\"Entering Transmatcher\")\n",
        "       q, h, w, d = query_en.size()\n",
        "       k, h, w, d = gallery_en.size()\n",
        "\n",
        "       # Reshaping to pass into encoder q,h,w,d to  hw, q, d\n",
        "       query_en_re = query_en.view(q, -1, d).permute(1, 0, 2)\n",
        "       gallery_en_re = gallery_en.view(k, -1, d).permute(1, 0, 2)\n",
        "\n",
        "       # Encoder 1\n",
        "       query_out_1 = self.encoder_layer1(query_en_re)\n",
        "       gallery_out_1 = self.encoder_layer1(gallery_en_re)\n",
        "\n",
        "       # Encoder 2\n",
        "       query_out_2 = self.encoder_layer2(query_out_1)\n",
        "       gallery_out_2 = self.encoder_layer2(gallery_out_1)\n",
        "       \n",
        "       # Decoder 1\n",
        "       score_0 = torch.zeros(q,k).cuda() #torch.zeros(q*k, 1) - initial score is 0\n",
        "       score_1 =  self.decoder_layer1(query_en, gallery_en, score_0)\n",
        "       \n",
        "       # Reshaping back to q, h, w, d to pass to decoder hw,q,d - q, h, w,d -\n",
        "       query_out_re1 = query_out_1.permute(1, 0, 2).reshape(q, h, w, -1)\n",
        "       gallery_out_re1 = gallery_out_1.permute(1, 0, 2).reshape(k, h, w, -1)\n",
        "\n",
        "       # Decoder 2\n",
        "       score_2 =  self.decoder_layer2(query_out_re1, gallery_out_re1, score_1)\n",
        "\n",
        "       # Reshaping back to q, h, w, d to pass to decoder hw,q,d - q, h, w,d -\n",
        "       query_out_re2 = query_out_2.permute(1, 0, 2).reshape(q, h, w, -1)\n",
        "       gallery_out_re2 = gallery_out_2.permute(1, 0, 2).reshape(k, h, w, -1)\n",
        "\n",
        "       # Decoder 3\n",
        "       score_out =  self.decoder_layer3(query_out_re2, gallery_out_re2, score_2)\n",
        "       return score_out"
      ],
      "metadata": {
        "id": "HJMyqSBXUcuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Resnet50-ibn-b Class**\n",
        "This class contains the resnet50-ibn-b from https://github.com/XingangPan/IBN-Net and the 3x3 neck convolution as explained in the paper\n",
        "- The query and gallery images are passed through the resnet50-ibn-b layer 3 (output dimensions 1024) and the corresponding feature embeddings are passed through an additional 3x3 neck convolution layer (output dimensions - 512) before being passed to the first encoder and the first decoder\n"
      ],
      "metadata": {
        "id": "aZsXby_2v5dL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResnetConv(Module):\n",
        "    def __init__(self, model):\n",
        "        super(ResnetConv, self).__init__()\n",
        "        self.neck_conv = nn.Conv2d(1024, 512, kernel_size=3, padding=1) # 3x3 neck convolution layer\n",
        "        self.model = model\n",
        "    def forward(self, inputs):\n",
        "        x = inputs\n",
        "        # getting the feature embeddings\n",
        "        for name, module in self.model._modules.items():\n",
        "            x = module(x)\n",
        "            if name == 'layer3':\n",
        "                break\n",
        "        # passing the feature embeddings to the 3x3 neck convolution layer\n",
        "        x = self.neck_conv(x)\n",
        "        x = x.permute(0,2,3,1) # changing order from q,d,h,w to q, h,w,d to pass to decoder and encoder since output of resnet is q,d,h,w\n",
        "        return x"
      ],
      "metadata": {
        "id": "jWq_SVj0v8ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Function to get training data and test data**\n",
        "These function definitions are reused from the paper implementation"
      ],
      "metadata": {
        "id": "7K8-qN5plWPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(dataname, data_dir, model, matcher, save_path, args):\n",
        "    root = osp.join(data_dir, dataname)\n",
        "\n",
        "    dataset = datasets.create(dataname, root, combine_all=False)\n",
        "\n",
        "    num_classes = dataset.num_train_ids\n",
        "\n",
        "    train_transformer = T.Compose([\n",
        "        T.Resize((args.height, args.width), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.Pad(10),\n",
        "        T.RandomCrop((args.height, args.width)),\n",
        "        T.RandomHorizontalFlip(0.5),\n",
        "        T.RandomRotation(5), \n",
        "        T.ColorJitter(brightness=(0.5, 2.0), contrast=(0.5, 2.0), saturation=(0.5, 2.0), hue=(-0.1, 0.1)),\n",
        "        T.RandomOcclusion(args.min_size, args.max_size),\n",
        "        T.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    test_transformer = T.Compose([\n",
        "        T.Resize((args.height, args.width), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    train_path = osp.join(dataset.images_dir, dataset.train_path)\n",
        "    train_loader = DataLoader(\n",
        "        Preprocessor(dataset.train, root=train_path, transform=train_transformer),\n",
        "        batch_size=args.batch_size, num_workers=args.workers,\n",
        "        sampler=GraphSampler(dataset.train, train_path, test_transformer, model, matcher, args.batch_size, args.num_instance,\n",
        "                    args.test_gal_batch, args.test_prob_batch, save_path, args.gs_verbose),\n",
        "        pin_memory=True)\n",
        "\n",
        "    query_loader = DataLoader(\n",
        "        Preprocessor(dataset.query,\n",
        "                     root=osp.join(dataset.images_dir, dataset.query_path), transform=test_transformer),\n",
        "        batch_size=args.test_fea_batch, num_workers=args.workers,\n",
        "        shuffle=False, pin_memory=True)\n",
        "\n",
        "    gallery_loader = DataLoader(\n",
        "        Preprocessor(dataset.gallery,\n",
        "                     root=osp.join(dataset.images_dir, dataset.gallery_path), transform=test_transformer),\n",
        "        batch_size=args.test_fea_batch, num_workers=args.workers,\n",
        "        shuffle=False, pin_memory=True)\n",
        "\n",
        "    return dataset, num_classes, train_loader, query_loader, gallery_loader\n"
      ],
      "metadata": {
        "id": "J13Fz-qXla_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_data(dataname, data_dir, height, width, workers=8, test_batch=64):\n",
        "    root = osp.join(data_dir, dataname)\n",
        "\n",
        "    dataset = datasets.create(dataname, root, combine_all=False)\n",
        "\n",
        "    test_transformer = T.Compose([\n",
        "        T.Resize((height, width), interpolation=3),\n",
        "        T.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    query_loader = DataLoader(\n",
        "        Preprocessor(dataset.query,\n",
        "                     root=osp.join(dataset.images_dir, dataset.query_path), transform=test_transformer),\n",
        "        batch_size=test_batch, num_workers=workers,\n",
        "        shuffle=False, pin_memory=True)\n",
        "\n",
        "    gallery_loader = DataLoader(\n",
        "        Preprocessor(dataset.gallery,\n",
        "                     root=osp.join(dataset.images_dir, dataset.gallery_path), transform=test_transformer),\n",
        "        batch_size=test_batch, num_workers=workers,\n",
        "        shuffle=False, pin_memory=True)\n",
        "\n",
        "    return dataset, query_loader, gallery_loader"
      ],
      "metadata": {
        "id": "fzFv_unjlNBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Training Loop**\n",
        "We have reused parts of this code from QAConv-GS and modified to it run our chosen experiment from the list of experiments provided in the paper\n"
      ],
      "metadata": {
        "id": "mv-wvbVyk6LJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args):\n",
        "\n",
        "    # Initializations\n",
        "    working_dir = os.path.dirname(os.path.abspath(\"OurTransformerDecoder.ipynb\"))\n",
        "    dataset='randperson'\n",
        "    exp_dir=osp.join(working_dir, 'Exp')\n",
        "    data_dir=osp.join(working_dir, 'data')\n",
        "    method='TransMatcher'\n",
        "    sub_method='res50-ibnb-layer3'\n",
        "    testset='cuhk03_np_detected'\n",
        "    batch_size=64\n",
        "    workers=8\n",
        "    height=384\n",
        "    width=128\n",
        "    lr = 0.005\n",
        "    lr_stepped = False\n",
        "    clip_value = 4\n",
        "    #loss_list = []\n",
        "    #accuracy = []\n",
        "    # gpu stuff\n",
        "    cudnn.deterministic = False\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # log files\n",
        "    exp_database_dir = osp.join(exp_dir, string.capwords(dataset))\n",
        "    output_dir = osp.join(exp_database_dir, method, sub_method)\n",
        "    log_file = osp.join(output_dir, 'log.txt')\n",
        "\n",
        "    # Arguments for TransMatcher class\n",
        "    seq_len = 24 * 8\n",
        "    d_model = 512\n",
        "    dim_feedforward = 2048\n",
        "\n",
        "    # Calling Transmatcher method\n",
        "    matcher = TransMatcher(seq_len, d_model, dim_feedforward).cuda()\n",
        "    # Resnet\n",
        "    resnet50_ibn = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_b', pretrained=True)    \n",
        "    # Criterion - Thier own loss function which we have taken from QAConv-GS as explained in the paper\n",
        "    backbone = ResnetConv(resnet50_ibn).cuda()\n",
        "    criterion = PairwiseMatchingLoss(matcher).cuda()\n",
        "\n",
        "    # Optimizer - https://pytorch.org/docs/stable/optim.html\n",
        "    # taken from authors code but made a few modifications\n",
        "    base_param_ids = set(map(id, backbone.parameters()))\n",
        "    new_params = [p for p in matcher.parameters() if\n",
        "                  id(p) not in base_param_ids]\n",
        "    param_groups = [\n",
        "        {'params': backbone.parameters(), 'lr': 0.0005} ,# The have specified in paper, lr for backbone network is 0.0005\n",
        "        {'params': new_params, 'lr': 0.005}]\n",
        "        #{'params': matcher.parameters(), 'lr': 0.005}\n",
        "\n",
        "    optimizer = torch.optim.SGD(param_groups, lr=lr, momentum=0.9)\n",
        "\n",
        "    # Load from checkpoint\n",
        "    start_epoch = 0\n",
        "\n",
        "    # parallel computation\n",
        "    model_bb = nn.DataParallel(backbone).cuda()\n",
        "    save_path = None\n",
        "\n",
        "    # Dataloader\n",
        "    dataset, num_classes, train_loader, _, _ = get_data(dataset, data_dir, model_bb, matcher, save_path, args)\n",
        "\n",
        "    # Training starts\n",
        "    # Trainer\n",
        "    trainer = Trainer(model_bb, criterion, clip_value)\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Start training - 15 epochs as mentioned in the paper\n",
        "    for epoch in range(start_epoch, 4):\n",
        "    # Decay learning rate by 0.1 after 10 epochs\n",
        "            if epoch == 2:\n",
        "                print('Decay the learning rate by a factor of 0.1.')\n",
        "                for group in optimizer.param_groups:\n",
        "                    group['lr'] *= 0.1\n",
        "            loss, acc = trainer.train(epoch, train_loader, optimizer)\n",
        "            print(loss)\n",
        "            #loss_list.append(loss) #adding loss to list\n",
        "            #accuracy.append(acc)\n",
        "            lr = list(map(lambda group: group['lr'], optimizer.param_groups))\n",
        "            #lr_scheduler.step()\n",
        "            optimizer.step()\n",
        "            train_time = time.time() - t0\n",
        "            epoch1 = epoch + 1\n",
        "\n",
        "            print(\n",
        "                '* Finished epoch %d at lr=[%g, %g]. Loss: %.3f. Acc: %.2f%%. Training time: %.0f seconds.\\n'\n",
        "                % (epoch1, lr[0], lr[1], loss, acc * 100, train_time)) #lr[2] %g\n",
        "            s = 'checkpoint_' + str(epoch1) + '.pth.tar'   \n",
        "            save_checkpoint({\n",
        "                'model': model_bb.module.state_dict(),\n",
        "                'criterion': criterion.state_dict(),\n",
        "                'optim': optimizer.state_dict(),\n",
        "                'epoch': epoch1,\n",
        "            }, fpath=osp.join(output_dir, s))\n",
        "            \n",
        "            # Saving results in JSON file        \n",
        "            json_file = osp.join(output_dir, 'results.json')\n",
        "            arg_dict = {'train_dataset': args.dataset, 'exp_dir': args.exp_dir, 'method': args.method, 'sub_method': args.sub_method}\n",
        "            with open(json_file, 'a') as f:\n",
        "                  json.dump(arg_dict, f)\n",
        "                  f.write('\\n')\n",
        "            train_dict = {'train_dataset': args.dataset, 'loss': loss, 'acc': acc, 'epochs': epoch1, 'train_time': train_time}\n",
        "            with open(json_file, 'a') as f:\n",
        "                  json.dump(train_dict, f)\n",
        "                  f.write('\\n')\n",
        "    # Training loop ends here\n",
        "\n",
        "    print('Finished training at epoch %d, loss = %.3f, acc = %.2f%%.\\n'\n",
        "              % (epoch1, loss, acc * 100))\n",
        "    print(\"Total training time: %.3f sec. Average training time per epoch: %.3f sec.\" % (\n",
        "            train_time, train_time / (epoch1 - start_epoch)))\n",
        " \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5cgMJs1tgY9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing loop**\n",
        "\n",
        "Certain parts reused from QAConv-GS and modified"
      ],
      "metadata": {
        "id": "n9aTuDv7S8lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# eval\n",
        "\n",
        "def eval_loop(args):\n",
        "  cudnn.deterministic = False\n",
        "  cudnn.benchmark = True\n",
        "  print('Evaluate the learned model:')\n",
        "  # log files - done\n",
        "  exp_database_dir = osp.join(args.exp_dir, string.capwords(args.dataset))\n",
        "  output_dir = osp.join(exp_database_dir, args.method, args.sub_method)\n",
        "  log_file = osp.join(output_dir, 'log.txt')\n",
        "  lr = 0.005\n",
        "\n",
        "  # Arguments for TransMatcher class - done\n",
        "  seq_len = 24 * 8\n",
        "  d_model = 512\n",
        "  dim_feedforward = 2048\n",
        "  # Loading checkpoint\n",
        "  checkpoint = load_checkpoint(osp.join(output_dir, 'checkpoint_15.pth.tar'))\n",
        " \n",
        "  # Calling Transmatcher method\n",
        "  matcher = TransMatcher(seq_len, d_model, dim_feedforward).cuda()\n",
        "    # Resnet\n",
        "  resnet50_ibn = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_b', pretrained=True)    \n",
        "    # In the paper they have used 3x3 neck convolution layer which is appended to the resnet50 ibn for further feature extraction\n",
        "    #neck_conv = nn.Conv2d(1024, 512, kernel_size=3, padding=1) #educated guess 512\n",
        "    # Criterion - Thier own loss function which we have taken from QAConv-GS as explained in the paper\n",
        "  backbone = ResnetConv(resnet50_ibn).cuda()\n",
        "  criterion = PairwiseMatchingLoss(matcher).cuda()\n",
        "  \n",
        "  save_path = None\n",
        "    # Optimizer - https://pytorch.org/docs/stable/optim.html\n",
        "    # taken from authors code but made a few modifications\n",
        "  base_param_ids = set(map(id, backbone.parameters()))\n",
        "  new_params = [p for p in matcher.parameters() if\n",
        "                  id(p) not in base_param_ids]\n",
        "  param_groups = [\n",
        "      {'params': backbone.parameters(), 'lr': 0.0005} ,# The have specified in paper, lr for backbone network is 0.0005\n",
        "      {'params': new_params, 'lr': 0.005}]\n",
        "        #{'params': matcher.parameters(), 'lr': 0.005}\n",
        "\n",
        "  optimizer = torch.optim.SGD(param_groups, lr=lr, momentum=0.9)\n",
        "\n",
        "  backbone.load_state_dict(checkpoint['model'])\n",
        "  criterion.load_state_dict(checkpoint['criterion'])\n",
        "  optimizer.load_state_dict(checkpoint['optim'])\n",
        "\n",
        "  model_bb = nn.DataParallel(backbone).cuda()\n",
        "  # Evaluator\n",
        "  evaluator = Evaluator(model_bb)\n",
        "  t0 = time.time()\n",
        "  test_names = args.testset.strip().split(',')\n",
        "  for test_name in test_names:\n",
        "      if test_name not in datasets.names():\n",
        "          print('Unknown dataset: %s.' % test_name)\n",
        "          continue\n",
        "\n",
        "  t1 = time.time()\n",
        "  testset, test_query_loader, test_gallery_loader = \\\n",
        "    get_test_data(test_name, args.data_dir, args.height, args.width, args.workers, args.test_fea_batch)\n",
        "\n",
        "  do_tlift = False\n",
        "  if not do_tlift:\n",
        "    testset.has_time_info = False\n",
        "\n",
        "  test_rank1, test_mAP, test_rank1_rerank, test_mAP_rerank, test_rank1_tlift, test_mAP_tlift, test_dist, \\\n",
        "    test_dist_rerank, test_dist_tlift, pre_tlift_dict = \\\n",
        "        evaluator.evaluate(matcher, testset, test_query_loader, test_gallery_loader, \n",
        "                                  args.test_gal_batch, args.test_prob_batch,\n",
        "                                args.tau, args.sigma, args.K, args.alpha)\n",
        "\n",
        "  test_time = time.time() - t1\n",
        "\n",
        "  test_dict = {'test_dataset': test_name, 'rank1': test_rank1, 'mAP': test_mAP, 'test_time': test_time}\n",
        "  print('  %s: rank1=%.1f, mAP=%.1f.\\n' % (test_name, test_rank1 * 100, test_mAP * 100))\n",
        "\n",
        "  json_file = osp.join(output_dir, 'eval.json')\n",
        "  with open(json_file, 'a') as f:\n",
        "      json.dump(test_dict, f)\n",
        "      f.write('\\n')\n",
        "\n",
        "  test_time = time.time() - t0"
      ],
      "metadata": {
        "id": "nPZcn480Y1NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Main function to call train and eval loop**"
      ],
      "metadata": {
        "id": "gXzAskSePFUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "import os\n",
        "import sys\n",
        "if __name__ == '__main__':\n",
        "# pass command line arguments directly in the code cell\n",
        "# epoch - 15 for market dataset and 4 for randperson as metioned in the paper\n",
        "# All the values of the arguments specified here are as exactly in the paper\n",
        "  working_dir = os.path.dirname(os.path.abspath(\"OurTransformerDecoder.ipynb\"))\n",
        "  args = argparse.Namespace(dataset='randperson', testset='cuhk03_np_detected', batch_size=64, workers=8, height=384, width=128,\n",
        "                            min_size=0, max_size=0.8, lr=0.005, epochs=4, step_size=2, clip_value=4, tau=100, sigma=200, K=10, alpha=0.2,\n",
        "                            num_instance=4, evaluate=False, test_fea_batch=256, test_gal_batch=128, test_prob_batch=128, data_dir=osp.join(working_dir, 'data'),\n",
        "                            exp_dir=osp.join(working_dir, 'Exp'), method='TransMatcher', sub_method='res50-ibnb-layer3', arch='resnet50',resume='', gs_save=False, combine_all=False, gs_verbose=False)\n",
        "\n",
        "  train(args)\n",
        "  eval_loop(args)"
      ],
      "metadata": {
        "id": "DfAqg__03pvc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}