# -*- coding: utf-8 -*-
"""Transformer Decoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WXWm6Dl3AWZx9owgpxve0uKMJHb6cSDx

##**Imports**
"""

! pip install torch
import copy
import torch
from torch import nn
from torch import Tensor
from torch.nn.init import xavier_uniform_
from torch.nn.modules import Module
from torch.nn.modules.container import ModuleList
from torch import einsum

"""##**TransformerDecoderLayer**
source : https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerDecoder
"""

class TransformerDecoderLayer(Module):
   __constants__ = ['batch_first', 'norm_first']
   # using default values for d_model (512) and dim_feedforward (2048) as mentioned in the paper
   def __init__(self, seq_len, d_model: int =512, dim_feedforward: int = 2048):
    super(TransformerDecoderLayer, self).__init__()
     # parameters
    self.seq_len = seq_len
    self.d_model = d_model

    # The prior score embeddings are learnable parameters of size hw Ã— hw. They can also be considered
    # learnable weights, somewhat similar to the learnable FC weights
   
    prior_score_weight = torch.randn(self.seq_len, self.seq_len)
    # creating prior score as a learnable parameter
    # Not sure why they are reshaping here
    self.learnable_prior_score_weight = nn.Parameter(prior_score_weight.view(1,1,self.seq_len, self.seq_len))

    # Instantiating all the layer according to figure 1 in the paper
    self.fc1 = torch.nn.Linear(d, d, bias=True)
    self.bn1 = torch.nn.BatchNorm1d(1)
    self.fc2 =  torch.nn.Linear(self.seq_len, dim_feedforward, bias=True)
    self.bn2 = torch.nn.BatchNorm1d(dim_feedforward)
    self.fc3 = torch.nn.Linear(dim_feedforward, 1, bias=True)
    self.bn3 = torch.nn.BatchNorm1d(1)
    self.relu = torch.nn.ReLU()

    # from the paper - we use shared FC parameters for both query and gallery, because they are exchangeable in the image 
    # matching task, and the similarity metric needs to be symmetrically defined - Does this mean same set of weights for both query and gallery  or same weights for all linear layers ?

   def forward(
        self,
        tgt: Tensor,
        memory: Tensor) -> Tensor:
    
    # tgt -> input to the decoder (assuming this is the output of the corresponding encoder layer)
    # memory -> input from the last encoder layer 
    # getting the value of parameters q k h w d as explained in the paper
    q, h, w, d = tgt.size()
    k, h, w, d = memory.size()

    # not sure why they are reshaping here:
    tgt = tgt.view(q,-1,d)
    memory = memory.view(k,-1,d)

    # passing tgt and memory through the fully connected layer to get query and gallery as explained in the paper
    query = self.fc1(tgt)
    key = self.fc1(memory)

    # NOTE - All this is present in QA conv so we can say we have just taken this from QA conv?
    # dot product (batched matrix multiplication) of query and gallery 
    mat_mul = einsum('q t d, k s d -> q k s t', query, key)

    # sigmoid of prior-score embedding
    score_sig = self.prior_score_encode.sigmoid()

    # element wise multiplication of dot product and output of sigmoid
    # Each element of score contains a pairwise similarity score between a position in the query sequence and a position in the key sequence
    final_score = mat_mul * score_sig

    # Reshape (q,k,self.seq_len, seq_len) to (q*k, self.seq_len, self.seq_len) i.e 4D to 3D
    final_score = final_score.reshape(q*k, self.seq_len, self.seq_len)

    # GMP layer as it is from the QA conv as explained in the paper
    final_score = torch.cat((final_score.max(dim=1)[0], final_score.max(dim=2)[0]), dim=-1)

    # Pass the score through each layer 
    final_score = final_score.view(-1,1,self.seq_len)
    final_score = self.bn1(final_score)
    final_score = final_score.view(-1, self.seq_len)
    final_score = self.fc2(final_score)
    final_score = self.bn2(final_score)
    final_score = self.relu(final_score)
    final_score = self.fc3(final_score)
    final_score = final_score.view(-1, 2).sum(dim=-1, keepdim=True)
    final_score = self.bn3(final_score)
    final_score = final_score.view(q, k)
    return final_score

"""##**Transformer Decoder**
TransformerDecoder is a stack of N decoder layers
"""

class TransformerDecoder(Module):
  # All this code is from the actual pytorch implementation of Transformer Decoder class
   __constants__ = ['norm']
   def __init__(self, decoder_layer, num_layers, norm=None):
     super().__init__()
     self.layers = _get_clones(decoder_layer, num_layers)
     self.num_layers = num_layers
     self.norm = norm
   def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:
      output = tgt

   # we have to modify this code to add score n-1 to current score if index is not 0
   for mod in self.layers:
      output = mod(output, memory)

   if self.norm is not None:
      output = self.norm(output)
   return output


def _get_clones(module, N):
    return ModuleList([copy.deepcopy(module) for i in range(N)])

"""##**TransMatcher class to call the decoder**

Similar to Transformer class in original pytorch implementation of Transformers
"""

# TransMatcher class to create the decoder
def __init__(self, seq_len, d_model=512, num_decoder_layers=3, dim_feedforward=2048):
        super().__init__()
        self.decoder_layer = TransformerDecoderLayer(seq_len, d_model, dim_feedforward)
        decoder_norm = nn.BatchNorm1d(1)
        self.decoder = TransformerDecoder(self.decoder_layer, num_decoder_layers, decoder_norm)
        self.memory = None
        self.seq_len = seq_len
        self.d_model = d_model
        self.reset_parameters()

    def reset_parameters(self):
        for p in self.parameters():
            if p.dim() > 1:
                xavier_uniform_(p)

# present in QA Conv
    def make_kernel(self, features):
        self.memory = features

    def forward(self, features):
        score = self.decoder(self.memory, features)
        return score

"""##**Main function to test**"""

# This is just a test function to see if the above implementation works and it does
if __name__ == "__main__":
    import time
    model = TransMatcher(24*8, 512, 3).eval()
    gallery = torch.rand((32, 24, 8, 512*3))
    probe = torch.rand((16, 24, 8, 512*3))

    start = time.time()
    model.make_kernel(gallery)
    out = model(probe)
    print(out.size())
    end = time.time()
    print('Time: %.3f seconds.' % (end - start))

    start = time.time()
    model.make_kernel(probe)
    out2 = model(gallery)
    print(out2.size())
    end = time.time()
    print('Time: %.3f seconds.' % (end - start))
    out2 = out2.t()
    print((out2 == out).all())
    print((out2 - out).abs().mean())
    print(out[:4, :4])
    print(out2[:4, :4])