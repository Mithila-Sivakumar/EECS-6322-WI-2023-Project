{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6kLwdGrX8jk"
      },
      "source": [
        "##**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IdGR2PXX_Ys",
        "outputId": "16b1d9b7-ace9-4a10-b694-8aca155a0585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/NN/QAConv\n"
          ]
        }
      ],
      "source": [
        "# Installs\n",
        "#!pip install torch\n",
        "#!pip install argparse\n",
        "\n",
        "#mounting google drive to load the datasets and the reid folders present in QAConv\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/MyDrive/NN/QAConv\n",
        "\n",
        "# Imports\n",
        "import copy\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import os.path as osp\n",
        "import sys\n",
        "import string\n",
        "import time\n",
        "import json\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.nn.modules import Module\n",
        "from torch.nn.modules.container import ModuleList\n",
        "from torch import einsum\n",
        "from google.colab import drive\n",
        "from __future__ import absolute_import\n",
        "from typing import Optional, Any\n",
        "\n",
        "\n",
        "\n",
        "from torch.nn import Module, ModuleList\n",
        "import torchvision\n",
        "from torch.nn.modules import TransformerEncoderLayer\n",
        "from __future__ import print_function, absolute_import\n",
        "from torch.backends import cudnn\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "#sys.path.append('QAConv')\n",
        "from reid import datasets\n",
        "from reid.trainers import Trainer\n",
        "from reid.evaluators import Evaluator\n",
        "from reid.utils.data import transforms as T\n",
        "from reid.utils.data.preprocessor import Preprocessor\n",
        "from reid.utils.logging import Logger\n",
        "from reid.utils.serialization import load_checkpoint, save_checkpoint\n",
        "\n",
        "from reid.utils.data.graph_sampler import GraphSampler\n",
        "from reid.loss.pairwise_matching_loss import PairwiseMatchingLoss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQuovx25YBka"
      },
      "source": [
        "##**TransformerDecoderLayer**\n",
        "source : https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerDecoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7j3vZLqYH9W"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderLayer(Module):\n",
        "   __constants__ = ['batch_first', 'norm_first']\n",
        "   # using default values for d_model (512) and dim_feedforward (2048) as mentioned in the paper\n",
        "   def __init__(self, seq_len, d_model: int = 512, dim_feedforward: int = 2048):\n",
        "    super(TransformerDecoderLayer, self).__init__()\n",
        "     # parameters\n",
        "    self.seq_len = seq_len\n",
        "    self.d_model = d_model\n",
        "\n",
        "    # The prior score embeddings are learnable parameters of size hw Ã— hw. They can also be considered\n",
        "    # learnable weights, somewhat similar to the learnable FC weights\n",
        "   \n",
        "    prior_score_weight = torch.randn(self.seq_len, self.seq_len)\n",
        "    # creating prior score as a learnable parameter\n",
        "    prior_score_weight = prior_score_weight.view(1,1,self.seq_len, self.seq_len)\n",
        "    self.learnable_prior_score_weight = nn.Parameter(prior_score_weight)\n",
        "\n",
        "    # Instantiating all the layer according to figure 1 in the paper\n",
        "    self.fc1 = torch.nn.Linear(d_model, d_model, bias=True)\n",
        "    self.bn1 = torch.nn.BatchNorm1d(2*self.seq_len)\n",
        "    self.mlphead = torch.nn.Sequential(\n",
        "                              torch.nn.Linear(2*self.seq_len, dim_feedforward),\n",
        "                              torch.nn.BatchNorm1d(dim_feedforward),\n",
        "                              torch.nn.ReLU(),\n",
        "                              torch.nn.Linear(dim_feedforward, 1))\n",
        "    self.bn3 = torch.nn.BatchNorm1d(1)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "\n",
        "    # from the paper - we use shared FC parameters for both query and gallery, because they are exchangeable in the image \n",
        "    # matching task, and the similarity metric needs to be symmetrically defined - Does this mean same set of weights for both query and gallery  or same weights for all linear layers ?\n",
        "\n",
        "   def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
        "    \n",
        "    # tgt -> input to the decoder (assuming this is the output of the corresponding encoder layer)\n",
        "    # memory -> input from the last encoder layer \n",
        "    # getting the value of parameters q k h w d as explained in the paper\n",
        "    q, h, w, d = tgt.size()\n",
        "    k, h, w, d = memory.size()\n",
        "\n",
        "    # Reshapig to prepare tgt and memory for matrix multioplication change tgt from q,h,w,d to q, h*w, d\n",
        "    # ie changing q,h,w,d to q,t,d\n",
        "    tgt = tgt.view(q,-1,d)\n",
        "    memory = memory.view(k,-1,d)\n",
        "\n",
        "    # passing tgt and memory through the fully connected layer to get query and gallery as explained in the paper\n",
        "    query = self.fc1(tgt)\n",
        "    gallery = self.fc1(memory)\n",
        "\n",
        "    # dot product (batched matrix multiplication) of query and gallery - taken from QA conv as explained in the paper\n",
        "    mat_mul = einsum('q t d, k s d -> q k s t', query, gallery)\n",
        "\n",
        "    # sigmoid of prior-score embedding\n",
        "    score_sig = self.learnable_prior_score_weight.sigmoid()\n",
        "\n",
        "    # element wise multiplication of dot product and output of sigmoid\n",
        "    # Each element of score contains a pairwise similarity score between a position in the query sequence and a position in the key sequence\n",
        "    final_score = mat_mul * score_sig\n",
        "\n",
        "    # Reshape (q,k,s,t) to (q*k, self.seq_len, self.seq_len)\n",
        "    final_score = final_score.reshape(q*k, self.seq_len, self.seq_len)\n",
        "\n",
        "    # GMP layer as it is from the QA conv as explained in the paper\n",
        "    final_score = torch.cat((final_score.max(dim=1)[0], final_score.max(dim=2)[0]), dim=-1)\n",
        "\n",
        "    # Pass the score through each layer \n",
        "    final_score = self.bn1(final_score)\n",
        "    final_score = self.mlphead(final_score)\n",
        "    final_score = self.bn3(final_score)\n",
        "    final_score = final_score.view(q, k) # check this - needs to be a scalar - from thier Transmatcher\n",
        "    return final_score\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmIFbBuT1nZa"
      },
      "source": [
        "##**Transformer Decoder**\n",
        "TransformerDecoder is a stack of N decoder layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1q3Se851rTr"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder1(Module):\n",
        "  # All this code is from the actual pytorch implementation of Transformer Decoder class\n",
        "   __constants__ = ['norm']\n",
        "   def __init__(self, decoder_layer, num_layers, norm=None):\n",
        "     super(TransformerDecoder, self).__init__()\n",
        "     self.layers = _get_clones(decoder_layer, num_layers)\n",
        "     self.num_layers = num_layers\n",
        "     self.norm = norm\n",
        "   def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
        "      output = tgt\n",
        "      print(self.layers)\n",
        "   # This did no work (without chunks) - i am doing something wrong\n",
        "      for i, mod in enumerate(self.layers):\n",
        "          if i == 0:\n",
        "            output = mod(output, memory)\n",
        "          else:\n",
        "            output = output + mod(output, memory) \n",
        "\n",
        "      if self.norm is not None:\n",
        "          output = self.norm(output)\n",
        "      return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpLAtcLSnC05"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(torch.nn.Module):\n",
        "  def __init__(self, decoder, num_of_layers, norm=None):\n",
        "    __constants__ = ['norm'] #present in the original code\n",
        "    super(TransformerDecoder, self).__init__()\n",
        "  # creating decoder stacks\n",
        "    self.num_of_layers = num_of_layers\n",
        "    self.layers = _get_clones(decoder, num_of_layers)\n",
        "    self.norm = norm  \n",
        "\n",
        "  # tgt - input to decoder (I think this is the output of each encoder parallel to decoder)\n",
        "  # memory - output from last encoder\n",
        "  def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
        "    \n",
        "    tgt = tgt.chunk(self.num_of_layers, dim = -1)\n",
        "    memory = memory.chunk(self.num_of_layers, dim = -1)\n",
        "    # Using enumerate to keep track of the indices as well\n",
        "    # This for loop is for saving the scores. for the first layer (when i ==0 ) the score is just the output of the decoder module, for the\n",
        "    # subsequent layers it is the output of the previous decoder (as explained in the paper where they add score with score n-1)\n",
        "    \n",
        "    for i, layer in enumerate(self.layers):\n",
        "      if i == 0:\n",
        "        score = layer(tgt[i], memory[i])\n",
        "      else:\n",
        "        score = score + layer(tgt[i], memory[i]) # If the layer is not the first layer then add the output of MLP head with previous layer score\n",
        "    # This code without chunks doesnt work. itthrows mat multiplication error. we need to inspect why\n",
        "    '''\n",
        "    score = 0\n",
        "    for layer in self.layers:\n",
        "       score = score + layer(tgt, memory)\n",
        "    '''\n",
        "    if self.norm is not None:\n",
        "      q, k = score.size()\n",
        "      # reshaping before passing to normalization as I faced errors if I did not\n",
        "      score = score.view(-1, 1)\n",
        "      score = self.norm(score)\n",
        "      # back to original shape after normalization\n",
        "      score = score.view(q, k)\n",
        "    return score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW0r9qqeOIZD"
      },
      "source": [
        "##**TransformerEncoder**\n",
        "\n",
        "Stack of Transformerencoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVixKOQ63V9c"
      },
      "outputs": [],
      "source": [
        "# Similar to TransformerEncoder in official implementation\n",
        "# try to remove clones and replace with 2 encoder layers - encoderlayer1 and encoderlayer2, encoderlayer2 ouput passed to decoder 1 2 3\n",
        "class TransformerEncoder(Module):\n",
        "  __constants__ = ['norm']\n",
        "  def __init__(self, encoder_layer, num_layers, norm=None, enable_nested_tensor=False, mask_check=False):\n",
        "      super().__init__()\n",
        "      self.layers = _get_clones(encoder_layer, num_layers)\n",
        "      self.num_layers = num_layers\n",
        "      self.norm = norm\n",
        "      self.enable_nested_tensor = enable_nested_tensor\n",
        "      self.mask_check = mask_check\n",
        "    \n",
        "  # forward method - passing the input through the encoders - pass th output of encoder1 to encoder2 and decoder2\n",
        "  def forward(self, src: Tensor, mask: Optional[Tensor] = None, key_mask: Tensor, is_causal: Optional[bool] = None) -> Tensor:\n",
        "      output = src\n",
        "      for mod in self.layers:\n",
        "        output = mod(output, mask, key_mask)\n",
        "      if self.norm is not None:\n",
        "        output = self.norm(output)\n",
        "      return output\n",
        "  \n",
        "# function definition from original Transformer Decoder implementation\n",
        "def _get_clones(module, N):\n",
        "    return ModuleList([copy.deepcopy(module) for i in range(N)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snhZro0-OPYT"
      },
      "source": [
        "##**TransMatcher class - Similar to Transformer class**\n",
        "This class will instantiate decoder, encoder and resnet50 as first encoder\n",
        "\n",
        "Reference - https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kL4WJH56J_-"
      },
      "outputs": [],
      "source": [
        "# Resnet as first encoder as explained in the paper\n",
        "# Dimensions for the layers in Resnet 50 - The same used in the paper\n",
        "feature_dims = {'layer2': 512, 'layer3': 1024, 'layer4': 2048}\n",
        "\n",
        "class TransMatcher(Module):\n",
        "    def __init__(self, seq_len, d_model: int = 512, nhead: int = 1, num_encoder_layers: int = 2, num_decoder_layers: int = 3, dim_feedforward: int = 2048,\n",
        "                 final_layer='layer3', neck: int = 512, dropout: float = 0.):\n",
        "          super(TransMatcher, self).__init__()\n",
        "          self.memory = None\n",
        "          self.seq_len = seq_len\n",
        "          self.d_model = d_model\n",
        "          self.final_layer = final_layer #hardcode?\n",
        "          self.nhead = nhead\n",
        "          self.encoder = None\n",
        "          #resnet50 with ibn as explained in the paper - https://github.com/XingangPan/IBN-Net\n",
        "          self.backbone = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_b', pretrained=True)\n",
        "          self.reset_parameters()\n",
        "\n",
        "          # setting the feature dimensions for layer 3 of resnet\n",
        "          layer3_fea = 1024 # for layer3\n",
        "\n",
        "          # In the paper they have used 3x3 neck convolution layer which is appended to the resnet50 ibn for further feature extraction\n",
        "          self.neck_conv = nn.Conv2d(layer3_few, neck, kernel_size=3, padding=1)\n",
        "          \n",
        "          # encoders\n",
        "          encoder_layer = TransformerEncoderLayer(self.d_model, self.nhead, dim_feedforward, dropout)\n",
        "          encoder_norm = None # None in author's implementation\n",
        "          self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
        "\n",
        "          # decoders\n",
        "          self.decoder_layer = TransformerDecoderLayer(seq_len, d_model, dim_feedforward)\n",
        "          decoder_norm = None # play around witgh later as authors have batchnorm here\n",
        "          self.decoder = TransformerDecoder(self.decoder_layer, num_decoder_layers, decoder_norm)\n",
        "\n",
        "    #present in original implementation\n",
        "     def reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                xavier_uniform_(p)\n",
        "        \n",
        "    # Not sure how to implement output of each encoder given to respective decoder\n",
        "     def forward(self, src: Tensor, tgt: Tensor) -> Tensor:\n",
        "          # passing query and gallery through the backbone network independently as explained in the paper\n",
        "          query = src\n",
        "          gallery = tgt\n",
        "\n",
        "          # backbone network - to get the feature map\n",
        "          for name, module in self.backbone._modules.items():\n",
        "            query = module(query)\n",
        "            gallery = module(gallery)\n",
        "            if name == self.final_layer:\n",
        "                break\n",
        "\n",
        "          # 3x3 convolution neck:\n",
        "          query = self.neck_conv(query)\n",
        "          gallery = self.neck_conv(gallery)\n",
        "\n",
        "          # passing it to encoder\n",
        "          #inputs = torch.cat(query, gallery) # since encoderlayer forward function is taking only 1 input (check official implementation)\n",
        "          #memory = self.encoder(inputs) # output of last encoder\n",
        "          # OR pass it seprately thro the encoder? - i think this makes sense from the diagram in the paper\n",
        "          out_query = self.encoder(query)\n",
        "          out_gallery = self.encoder(gallery)\n",
        "         \n",
        "          \n",
        "          # how will we split query and gallery again here ????\n",
        "          # passing to decoder (this is confusing as well)\n",
        "          score = self.decoder(out_query, out_gallery) \n",
        "          return score\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttB2pjUO3wr4"
      },
      "source": [
        "##**TransMatcher class to call the decoder - This is based on paper DONT USE THIS**\n",
        "\n",
        "Similar to Transformer class in original pytorch implementation of Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ie9XaDNR32sH"
      },
      "outputs": [],
      "source": [
        "# TransMatcher class to create the decoder\n",
        "class TransMatcher(Module):\n",
        "  def __init__(self, seq_len, d_model=512, num_decoder_layers=3, dim_feedforward=2048, final_layer='layer3', neck=512, nhead=1, dropout: float = 0.):\n",
        "          super(TransMatcher, self).__init__()\n",
        "          self.decoder_layer = TransformerDecoderLayer(seq_len, d_model, dim_feedforward)\n",
        "          decoder_norm = nn.BatchNorm1d(1)\n",
        "          self.decoder = TransformerDecoder(self.decoder_layer, num_decoder_layers, decoder_norm)\n",
        "          self.memory = None\n",
        "          self.seq_len = seq_len\n",
        "          self.d_model = d_model\n",
        "          self.base = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_b', pretrained=True)\n",
        "          self.reset_parameters()\n",
        "\n",
        "#present in original implementation\n",
        "  def reset_parameters(self):\n",
        "      for p in self.parameters():\n",
        "                  if p.dim() > 1:\n",
        "                      xavier_uniform_(p)\n",
        "\n",
        "  # present in QA Conv - just mapping memory to gallery\n",
        "  def make_kernel(self, features):\n",
        "      self.memory = features\n",
        "\n",
        "  def forward(self, features):\n",
        "      score = self.decoder(self.memory, features)\n",
        "      return score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JvzByw77Yz9"
      },
      "source": [
        "##**Main function to test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jbKHvuu7b0m",
        "outputId": "42edbcd8-e78b-4dce-cfcc-777d67748f1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 16])\n",
            "Time: 2.366 seconds.\n",
            "torch.Size([16, 32])\n",
            "Time: 2.945 seconds.\n",
            "tensor(False)\n",
            "tensor(3.0053, grad_fn=<MeanBackward0>)\n",
            "tensor([[55.0166, 59.3318, 57.9900, 53.6307],\n",
            "        [56.9376, 61.9847, 58.2995, 57.2540],\n",
            "        [56.7064, 62.1524, 58.6920, 57.0373],\n",
            "        [54.7663, 57.7237, 55.7484, 54.7164]], grad_fn=<SliceBackward0>)\n",
            "tensor([[56.8486, 58.4968, 58.7244, 52.1372],\n",
            "        [61.8187, 65.9041, 61.4339, 59.3565],\n",
            "        [53.7679, 58.8465, 56.2316, 51.6803],\n",
            "        [52.6982, 55.1715, 55.3523, 49.5685]], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# This is just a test function to see if the above implementation works and it does\n",
        "if __name__ == \"__main__\":\n",
        "    import time\n",
        "    model = TransMatcher(24*8, 512, 3).eval()\n",
        "    gallery = torch.rand((32, 24, 8, 512*3))\n",
        "    probe = torch.rand((16, 24, 8, 512*3))\n",
        "\n",
        "    start = time.time()\n",
        "    model.make_kernel(gallery)\n",
        "    out = model(probe)\n",
        "    print(out.size())\n",
        "    end = time.time()\n",
        "    print('Time: %.3f seconds.' % (end - start))\n",
        "\n",
        "    start = time.time()\n",
        "    model.make_kernel(probe)\n",
        "    out2 = model(gallery)\n",
        "    print(out2.size())\n",
        "    end = time.time()\n",
        "    print('Time: %.3f seconds.' % (end - start))\n",
        "    out2 = out2.t()\n",
        "    print((out2 == out).all())\n",
        "    print((out2 - out).abs().mean())\n",
        "    print(out[:4, :4])\n",
        "    print(out2[:4, :4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BjqBZAbl-zu"
      },
      "source": [
        "##**Transformer Encoder & Resnt50IBN class**\n",
        "\n",
        "We have reused parts of code from the author's implementation and modified it to use just Resnet50withIBN since we have chosen to reproduce this experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOZInioyl3vr"
      },
      "outputs": [],
      "source": [
        "# Proper Implementation\n",
        "\n",
        "# Transformer Encoder\n",
        "# From pytorch official documentation\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html\n",
        "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "\n",
        "class TransformerEncoder(torch.nn.Module):\n",
        "    def __init__(self, encoder_layer, num_layer, norm=None):\n",
        "      super(TransformerEncoder, self).__init__()\n",
        "      self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for i in range(num_layer)])\n",
        "      self.num_layer = num_layer\n",
        "      self.norm = norm\n",
        "\n",
        "    # src - input sequence to the encoder src: Tensor, shape [seq_len, batch_size]\n",
        "    # src_mask: Tensor, shape [seq_len, seq_len]\n",
        "    # key_mask: The mask for the keys per batch\n",
        "    def forward(self, src: Tensor, src_mask: Tensor, key_mask: Tensor) -> Tensor:\n",
        "      out = src\n",
        "      out_list = []\n",
        "      for layer in self.layers:\n",
        "        out = layer(out, src_mask=src_mask, key_mask=key_mask)\n",
        "        out_list.append(out)\n",
        "      # applying normalization to the outputs if norm is not None\n",
        "      if self.norm is not None:\n",
        "        for i in range(len(out_list)):\n",
        "          out[i] = self.norm(out[i])\n",
        "      # concatenate the outputs into 1 tensor\n",
        "      output = torch.cat(out_list, dim=-1)\n",
        "      return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EYd3iLFfk7I"
      },
      "outputs": [],
      "source": [
        "# Resnet as first encoder as explained in the paper\n",
        "# Dimensions for the layers in Resnet 50 - The same used in the paper\n",
        "feature_dims = {'layer2': 512, 'layer3': 1024, 'layer4': 2048}\n",
        "class ResnetIBN(nn.Module):\n",
        "\n",
        "  # final_layer - default value is layer3 which is the third layer in resnet50\n",
        "  # nhead - number of multi head attention in the encoder\n",
        "  def __init__(self, final_layer='layer3', neck=512, nhead=1, num_encoder_layers=2, dim_feedforward=2048, dropout=0., pretrained=True):\n",
        "    super(ResnetIBN, self).__init__()\n",
        "    self.final_layer = final_layer\n",
        "    self.neck = neck\n",
        "    self.pretrained = pretrained\n",
        "    #resnet50 with ibn as explained in the paper - https://github.com/XingangPan/IBN-Net\n",
        "    self.base = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_b', pretrained=True)\n",
        "\n",
        "    # setting the feature dimensions for layer 3 of resnet\n",
        "    layer3_fea = feature_dims[final_layer]\n",
        "\n",
        "    # The purpose of the neck layers is to further refine the features extracted by the earlier layers, and to reduce the dimensionality of the feature vectors to a level that is suitable for the task at hand\n",
        "    # In the paper they have used 3x3 neck convolution layer which is appended to the resnet50 ibn for further feature extraction\n",
        "    if neck > 0:\n",
        "        self.neck_conv = nn.Conv2d(layer3_fea, neck, kernel_size=3, padding=1)\n",
        "        embeddings = neck\n",
        "\n",
        "    self.encoder = None\n",
        "    encoder_layer = TransformerEncoderLayer(embeddings, nhead, dim_feedforward, dropout)\n",
        "    encoder_norm = None\n",
        "    # stacking encoders (in this case 2 encoders)\n",
        "    self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
        "    self.num_features = embeddings\n",
        "\n",
        "  def forward(self, inputs):\n",
        "        # passing the inputs through the backbone layer to get the feature encodings\n",
        "        x = inputs\n",
        "        for name, module in self.base._modules.items():\n",
        "            x = module(x)\n",
        "            if name == self.final_layer:\n",
        "                break\n",
        "\n",
        "        if self.neck > 0:\n",
        "            x = self.neck_conv(x)\n",
        "\n",
        "        # passing the feature encodings to rest of the encoders.\n",
        "        out = x.permute(0, 2, 3, 1)  # [b, h, w, c]\n",
        "        if self.encoder is not None:\n",
        "            b, c, h, w = x.size()\n",
        "            y = x.view(b, c, -1).permute(2, 0, 1)  # [hw, b, c]\n",
        "            y = self.encoder(y)\n",
        "            y = y.permute(1, 0, 2).reshape(b, h, w, -1)  # [b, h, w, c]\n",
        "            out = torch.cat((out, y), dim=-1)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K8-qN5plWPs"
      },
      "source": [
        "##**Function to get training data and test data**\n",
        "These function definitions are reused from the paper implementation\n",
        "WE CAN USE THIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J13Fz-qXla_w"
      },
      "outputs": [],
      "source": [
        "def get_data(dataname, data_dir, model, matcher, save_path, args):\n",
        "    root = osp.join(data_dir, dataname)\n",
        "\n",
        "    dataset = datasets.create(dataname, root, combine_all=args.combine_all)\n",
        "\n",
        "    num_classes = dataset.num_train_ids\n",
        "\n",
        "    train_transformer = T.Compose([\n",
        "        T.Resize((args.height, args.width), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.Pad(10),\n",
        "        T.RandomCrop((args.height, args.width)),\n",
        "        T.RandomHorizontalFlip(0.5),\n",
        "        T.RandomRotation(5), \n",
        "        T.ColorJitter(brightness=(0.5, 2.0), contrast=(0.5, 2.0), saturation=(0.5, 2.0), hue=(-0.1, 0.1)),\n",
        "        T.RandomOcclusion(args.min_size, args.max_size),\n",
        "        T.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    test_transformer = T.Compose([\n",
        "        T.Resize((args.height, args.width), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    train_path = osp.join(dataset.images_dir, dataset.train_path)\n",
        "    train_loader = DataLoader(\n",
        "        Preprocessor(dataset.train, root=train_path, transform=train_transformer),\n",
        "        batch_size=args.batch_size, num_workers=args.workers,\n",
        "        sampler=GraphSampler(dataset.train, train_path, test_transformer, model, matcher, args.batch_size, args.num_instance,\n",
        "                    args.test_gal_batch, args.test_prob_batch, save_path, args.gs_verbose),\n",
        "        pin_memory=True)\n",
        "\n",
        "    query_loader = DataLoader(\n",
        "        Preprocessor(dataset.query,\n",
        "                     root=osp.join(dataset.images_dir, dataset.query_path), transform=test_transformer),\n",
        "        batch_size=args.test_fea_batch, num_workers=args.workers,\n",
        "        shuffle=False, pin_memory=True)\n",
        "\n",
        "    gallery_loader = DataLoader(\n",
        "        Preprocessor(dataset.gallery,\n",
        "                     root=osp.join(dataset.images_dir, dataset.gallery_path), transform=test_transformer),\n",
        "        batch_size=args.test_fea_batch, num_workers=args.workers,\n",
        "        shuffle=False, pin_memory=True)\n",
        "\n",
        "    return dataset, num_classes, train_loader, query_loader, gallery_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzFv_unjlNBJ"
      },
      "outputs": [],
      "source": [
        "def get_test_data(dataname, data_dir, height, width, workers=8, test_batch=64):\n",
        "    root = osp.join(data_dir, dataname)\n",
        "\n",
        "    dataset = datasets.create(dataname, root, combine_all=False)\n",
        "\n",
        "    test_transformer = T.Compose([\n",
        "        T.Resize((height, width), interpolation=3),\n",
        "        T.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    query_loader = DataLoader(\n",
        "        Preprocessor(dataset.query,\n",
        "                     root=osp.join(dataset.images_dir, dataset.query_path), transform=test_transformer),\n",
        "        batch_size=test_batch, num_workers=workers,\n",
        "        shuffle=False, pin_memory=True)\n",
        "\n",
        "    gallery_loader = DataLoader(\n",
        "        Preprocessor(dataset.gallery,\n",
        "                     root=osp.join(dataset.images_dir, dataset.gallery_path), transform=test_transformer),\n",
        "        batch_size=test_batch, num_workers=workers,\n",
        "        shuffle=False, pin_memory=True)\n",
        "\n",
        "    return dataset, query_loader, gallery_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv-wvbVyk6LJ"
      },
      "source": [
        "##**Main Function - ALREADY MODIFIED BASED ON OUR EXPERIMENT _ CAN USE THIS**\n",
        "We have reused parts of this code from the author's implemention and modified to it run our chosen experiment from the list of experiments provided in the paper\n",
        "\n",
        "NOTE - We need to modify it a bit more after adding encoder and transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cgMJs1tgY9K"
      },
      "outputs": [],
      "source": [
        "def main(args):\n",
        "    cudnn.deterministic = False\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    exp_database_dir = osp.join(args.exp_dir, string.capwords(args.dataset))\n",
        "    output_dir = osp.join(exp_database_dir, args.method, args.sub_method)\n",
        "    log_file = osp.join(output_dir, 'log.txt')\n",
        "    # Redirect print to both console and log file\n",
        "    sys.stdout = Logger(log_file)\n",
        "\n",
        "   # Creating the base network - resnet50 with ibn and also creating encoder layers. In the paper they mention that they have used only N-1 encoders (2 in this case)\n",
        "   # as they have seen slight improvement in the performance.The first encoder layer is resnet50 itself.\n",
        "\n",
        "    feamap_factor = {'layer3': 16}\n",
        "    # since feature map factor layer 3 is 16 as in the authors code\n",
        "    hei = args.height // 16\n",
        "    wid = args.width // 16\n",
        "    # change the arguments to transmatcher after final implementation\n",
        "    matcher = TransMatcher(hei * wid, num_features, args.num_trans_layers, args.dim_feedforward).cuda()\n",
        "\n",
        "\n",
        "    # Criterion - Thier own loss function which we have taken from QAConv-GS as explained in the paper\n",
        "    criterion = PairwiseMatchingLoss(matcher).cuda()\n",
        "\n",
        "    # Optimizer - https://pytorch.org/docs/stable/optim.html\n",
        "    base_param_ids = set(map(id, model.base.parameters()))\n",
        "    new_params = [p for p in model.parameters() if\n",
        "                  id(p) not in base_param_ids]\n",
        "    param_groups = [\n",
        "        {'params': model.base.parameters(), 'lr': 0.1 * args.lr} ,# The have specified in paper, lr for backbone network is 0.0005\n",
        "        {'params': new_params, 'lr': args.lr},\n",
        "        {'params': matcher.parameters(), 'lr': args.lr}]\n",
        "\n",
        "    optimizer = torch.optim.SGD(param_groups, lr=args.lr, momentum=0.9)\n",
        "\n",
        "    # Load from checkpoint\n",
        "    start_epoch = 0\n",
        "\n",
        "    if args.resume or args.evaluate:\n",
        "        print('Loading checkpoint...')\n",
        "        if args.resume and (args.resume != 'ori'):\n",
        "            checkpoint = load_checkpoint(args.resume)\n",
        "        else:\n",
        "            checkpoint = load_checkpoint(osp.join(output_dir, 'checkpoint.pth.tar'))\n",
        "        model.load_state_dict(checkpoint['model'])\n",
        "        criterion.load_state_dict(checkpoint['criterion'])\n",
        "        optimizer.load_state_dict(checkpoint['optim'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "\n",
        "        print(\"=> Start epoch {} \".format(start_epoch))\n",
        "\n",
        "    model = nn.DataParallel(model).cuda()\n",
        "\n",
        "    # Create data loaders\n",
        "    save_path = None\n",
        "    if args.gs_save:\n",
        "        save_path = output_dir\n",
        "    dataset, num_classes, train_loader, _, _ = get_data(args.dataset, args.data_dir, model, matcher, save_path, args)\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every step_size epochs\n",
        "    lr_scheduler = StepLR(optimizer, step_size=args.step_size, gamma=0.1, last_epoch=start_epoch-1)\n",
        "\n",
        "    if not args.evaluate:\n",
        "        # Trainer\n",
        "        trainer = Trainer(model, criterion, args.clip_value)\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Start training\n",
        "        for epoch in range(start_epoch, args.epochs):\n",
        "            loss, acc = trainer.train(epoch, train_loader, optimizer)\n",
        "\n",
        "            lr = list(map(lambda group: group['lr'], optimizer.param_groups))\n",
        "            lr_scheduler.step()\n",
        "            train_time = time.time() - t0\n",
        "            epoch1 = epoch + 1\n",
        "\n",
        "            print(\n",
        "                '* Finished epoch %d at lr=[%g, %g, %g]. Loss: %.3f. Acc: %.2f%%. Training time: %.0f seconds.                  \\n'\n",
        "                % (epoch1, lr[0], lr[1], lr[2], loss, acc * 100, train_time))\n",
        "\n",
        "            save_checkpoint({\n",
        "                'model': model.module.state_dict(),\n",
        "                'criterion': criterion.state_dict(),\n",
        "                'optim': optimizer.state_dict(),\n",
        "                'epoch': epoch1,\n",
        "            }, fpath=osp.join(output_dir, 'checkpoint.pth.tar'))\n",
        "\n",
        "    json_file = osp.join(output_dir, 'results.json')\n",
        "    \n",
        "    if not args.evaluate:\n",
        "        arg_dict = {'train_dataset': args.dataset, 'exp_dir': args.exp_dir, 'method': args.method, 'sub_method': args.sub_method}\n",
        "        with open(json_file, 'a') as f:\n",
        "            json.dump(arg_dict, f)\n",
        "            f.write('\\n')\n",
        "        train_dict = {'train_dataset': args.dataset, 'loss': loss, 'acc': acc, 'epochs': epoch1, 'train_time': train_time}\n",
        "        with open(json_file, 'a') as f:\n",
        "            json.dump(train_dict, f)\n",
        "            f.write('\\n')\n",
        "\n",
        "    # Final test\n",
        "    print('Evaluate the learned model:')\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Evaluator\n",
        "    evaluator = Evaluator(model)\n",
        "\n",
        "    test_names = args.testset.strip().split(',')\n",
        "    for test_name in test_names:\n",
        "        if test_name not in datasets.names():\n",
        "            print('Unknown dataset: %s.' % test_name)\n",
        "            continue\n",
        "\n",
        "        t1 = time.time()\n",
        "        testset, test_query_loader, test_gallery_loader = \\\n",
        "            get_test_data(test_name, args.data_dir, args.height, args.width, args.workers, args.test_fea_batch)\n",
        "\n",
        "        if not args.do_tlift:\n",
        "            testset.has_time_info = False\n",
        "\n",
        "        test_rank1, test_mAP, test_rank1_rerank, test_mAP_rerank, test_rank1_tlift, test_mAP_tlift, test_dist, \\\n",
        "        test_dist_rerank, test_dist_tlift, pre_tlift_dict = \\\n",
        "            evaluator.evaluate(matcher, testset, test_query_loader, test_gallery_loader, \n",
        "                                args.test_gal_batch, args.test_prob_batch,\n",
        "                               args.tau, args.sigma, args.K, args.alpha)\n",
        "\n",
        "        test_time = time.time() - t1\n",
        "\n",
        "        test_dict = {'test_dataset': test_name, 'rank1': test_rank1, 'mAP': test_mAP, 'test_time': test_time}\n",
        "        print('  %s: rank1=%.1f, mAP=%.1f.\\n' % (test_name, test_rank1 * 100, test_mAP * 100))\n",
        "\n",
        "        with open(json_file, 'a') as f:\n",
        "            json.dump(test_dict, f)\n",
        "            f.write('\\n')\n",
        "\n",
        "    test_time = time.time() - t0\n",
        "\n",
        "    if not args.evaluate:\n",
        "        print('Finished training at epoch %d, loss = %.3f, acc = %.2f%%.\\n'\n",
        "              % (epoch1, loss, acc * 100))\n",
        "        print(\"Total training time: %.3f sec. Average training time per epoch: %.3f sec.\" % (\n",
        "            train_time, train_time / (epoch1 - start_epoch)))\n",
        "    print(\"Total testing time: %.3f sec.\\n\" % test_time)\n",
        "\n",
        "\n",
        "%%capture\n",
        "# pass command line arguments directly in the code cell\n",
        "# epoch - 15 for market dataset and 4 for randperson as metioned in the paper\n",
        "# All the values of the arguments specified here are as exactly in the paper\n",
        "working_dir = os.path.dirname(os.path.abspath(\"main.ipynb\"))\n",
        "args = argparse.Namespace(dataset='market', testset='cuhk03_np_detected', batch_size=64, workers=8, height=384, width=128, final_layer='layer3', neck=512,\n",
        "                          ibn='b', nhead=1, num_trans_layers=3, dim_feedforward=2048, min_size=0, max_size=0.8, lr=0.005, epochs=15, step_size=10, clip_value=4,\n",
        "                          num_instance=4, evaluate=False, test_fea_batch=256, test_gal_batch=128, test_prob_batch=128, data_dir=osp.join(working_dir, 'data'),\n",
        "                         exp_dir=osp.join(working_dir, 'Exp'), method='TransMatcher', sub_method='res50-ibnb-layer3', arch='resnet50',resume='', gs_save=False, combine_all=False, gs_verbose=True)\n",
        "\n",
        "main(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main(args):\n",
        "    # gpu stuff\n",
        "    cudnn.deterministic = False\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # log files\n",
        "    exp_database_dir = osp.join(args.exp_dir, string.capwords(args.dataset))\n",
        "    output_dir = osp.join(exp_database_dir, args.method, args.sub_method)\n",
        "    log_file = osp.join(output_dir, 'log.txt')\n",
        "\n",
        "    # since feature map factor layer 3 is 16 as in the authors code\n",
        "    hei = args.height // 16\n",
        "    wid = args.width // 16\n",
        "    \n",
        "    # Arguments for TransMatcher class\n",
        "    seq_len = hei * wid\n",
        "    d_model = 512\n",
        "    nhead = 1\n",
        "    num_encoder_layers = 2\n",
        "    num_decoder_layers = 3\n",
        "    dim_feedforward = 2048\n",
        "    dropout = 0.\n",
        "\n",
        "     # Calling Transmatcher method\n",
        "    matcher = TransMatcher(dseq_len, _model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout).cuda()\n",
        "\n",
        "    # Criterion - Thier own loss function which we have taken from QAConv-GS as explained in the paper\n",
        "    criterion = PairwiseMatchingLoss(matcher).cuda()\n",
        "\n",
        "     # Optimizer - https://pytorch.org/docs/stable/optim.html\n",
        "     # taken from authors code but made a few modifications\n",
        "    base_param_ids = set(map(id, model.base.parameters()))\n",
        "    new_params = [p for p in model.parameters() if\n",
        "                  id(p) not in base_param_ids]\n",
        "    param_groups = [\n",
        "        {'params': model.base.parameters(), 'lr': 0.1 * args.lr} ,# The have specified in paper, lr for backbone network is 0.0005\n",
        "        {'params': new_params, 'lr': args.lr},\n",
        "        {'params': matcher.parameters(), 'lr': args.lr}]\n",
        "\n",
        "    optimizer = torch.optim.SGD(param_groups, lr=args.lr, momentum=0.9)\n",
        "\n",
        "    # Load from checkpoint\n",
        "    start_epoch = 0\n",
        "\n",
        "    #This part - check\n",
        "\n",
        "    if args.resume or args.evaluate:\n",
        "        print('Loading checkpoint...')\n",
        "        if args.resume and (args.resume != 'ori'):\n",
        "            checkpoint = load_checkpoint(args.resume)\n",
        "        else:\n",
        "            checkpoint = load_checkpoint(osp.join(output_dir, 'checkpoint.pth.tar'))\n",
        "        model.load_state_dict(checkpoint['model'])\n",
        "        criterion.load_state_dict(checkpoint['criterion'])\n",
        "        optimizer.load_state_dict(checkpoint['optim'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "\n",
        "        print(\"=> Start epoch {} \".format(start_epoch))\n",
        "\n",
        "    # parallel computation\n",
        "    model = nn.DataParallel(model).cuda()\n",
        "\n",
        "    # Create data loaders\n",
        "    # Same as in author's code\n",
        "    save_path = None\n",
        "    if args.gs_save:\n",
        "        save_path = output_dir\n",
        "    dataset, num_classes, train_loader, _, _ = get_data(args.dataset, args.data_dir, model, matcher, save_path, args)\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every step_size epochs\n",
        "    lr_scheduler = StepLR(optimizer, step_size=args.step_size, gamma=0.1, last_epoch=start_epoch-1)\n",
        "\n",
        "    # Training starts\n",
        "    if not args.evaluate:\n",
        "        # Trainer\n",
        "        trainer = Trainer(model, criterion, args.clip_value)\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Start training - 15 epochs as mentioned in the paper\n",
        "        for epoch in range(start_epoch, 15):\n",
        "            loss, acc = trainer.train(epoch, train_loader, optimizer)\n",
        "\n",
        "            lr = list(map(lambda group: group['lr'], optimizer.param_groups))\n",
        "            lr_scheduler.step()\n",
        "            train_time = time.time() - t0\n",
        "            epoch1 = epoch + 1\n",
        "\n",
        "            print(\n",
        "                '* Finished epoch %d at lr=[%g, %g, %g]. Loss: %.3f. Acc: %.2f%%. Training time: %.0f seconds.                  \\n'\n",
        "                % (epoch1, lr[0], lr[1], lr[2], loss, acc * 100, train_time))\n",
        "\n",
        "            save_checkpoint({\n",
        "                'model': model.module.state_dict(),\n",
        "                'criterion': criterion.state_dict(),\n",
        "                'optim': optimizer.state_dict(),\n",
        "                'epoch': epoch1,\n",
        "            }, fpath=osp.join(output_dir, 'checkpoint.pth.tar'))\n",
        "\n",
        "    json_file = osp.join(output_dir, 'results.json')\n",
        "    \n",
        "    if not args.evaluate:\n",
        "        arg_dict = {'train_dataset': args.dataset, 'exp_dir': args.exp_dir, 'method': args.method, 'sub_method': args.sub_method}\n",
        "        with open(json_file, 'a') as f:\n",
        "            json.dump(arg_dict, f)\n",
        "            f.write('\\n')\n",
        "        train_dict = {'train_dataset': args.dataset, 'loss': loss, 'acc': acc, 'epochs': epoch1, 'train_time': train_time}\n",
        "        with open(json_file, 'a') as f:\n",
        "            json.dump(train_dict, f)\n",
        "            f.write('\\n')\n",
        "\n",
        "    # Evaluation starts\n",
        "    print('Evaluate the learned model:')\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Evaluator\n",
        "    evaluator = Evaluator(model)\n",
        "\n",
        "    test_names = args.testset.strip().split(',')\n",
        "    for test_name in test_names:\n",
        "        if test_name not in datasets.names():\n",
        "            print('Unknown dataset: %s.' % test_name)\n",
        "            continue\n",
        "\n",
        "        t1 = time.time()\n",
        "        testset, test_query_loader, test_gallery_loader = \\\n",
        "            get_test_data(test_name, args.data_dir, args.height, args.width, args.workers, args.test_fea_batch)\n",
        "\n",
        "        if not args.do_tlift:\n",
        "            testset.has_time_info = False\n",
        "\n",
        "        test_rank1, test_mAP, test_rank1_rerank, test_mAP_rerank, test_rank1_tlift, test_mAP_tlift, test_dist, \\\n",
        "        test_dist_rerank, test_dist_tlift, pre_tlift_dict = \\\n",
        "            evaluator.evaluate(matcher, testset, test_query_loader, test_gallery_loader, \n",
        "                                args.test_gal_batch, args.test_prob_batch,\n",
        "                               args.tau, args.sigma, args.K, args.alpha)\n",
        "\n",
        "        test_time = time.time() - t1\n",
        "\n",
        "        test_dict = {'test_dataset': test_name, 'rank1': test_rank1, 'mAP': test_mAP, 'test_time': test_time}\n",
        "        print('  %s: rank1=%.1f, mAP=%.1f.\\n' % (test_name, test_rank1 * 100, test_mAP * 100))\n",
        "\n",
        "        with open(json_file, 'a') as f:\n",
        "            json.dump(test_dict, f)\n",
        "            f.write('\\n')\n",
        "\n",
        "    test_time = time.time() - t0\n",
        "\n",
        "    if not args.evaluate:\n",
        "        print('Finished training at epoch %d, loss = %.3f, acc = %.2f%%.\\n'\n",
        "              % (epoch1, loss, acc * 100))\n",
        "        print(\"Total training time: %.3f sec. Average training time per epoch: %.3f sec.\" % (\n",
        "            train_time, train_time / (epoch1 - start_epoch)))\n",
        "    print(\"Total testing time: %.3f sec.\\n\" % test_time)\n",
        "\n",
        "\n",
        "    %%capture\n",
        "#!pip install argparse\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# pass command line arguments directly in the code cell\n",
        "# epoch - 15 for market dataset and 4 for randperson as metioned in the paper\n",
        "# All the values of the arguments specified here are as exactly in the paper\n",
        "working_dir = os.path.dirname(os.path.abspath(\"main.ipynb\"))\n",
        "args = argparse.Namespace(dataset='market', testset='cuhk03_np_detected', batch_size=64, workers=8, height=384, width=128,\n",
        "                          min_size=0, max_size=0.8, lr=0.005, epochs=15, step_size=10, clip_value=4, tau=100, sigma=200, K=10, alpha=0.2, test_fea_batch=128, test_gal_batch=128, test_prob_batch=128,\n",
        "                          num_instance=4, evaluate=False, test_fea_batch=256, test_gal_batch=128, test_prob_batch=128, data_dir=osp.join(working_dir, 'data'),\n",
        "                          exp_dir=osp.join(working_dir, 'Exp'), method='TransMatcher', sub_method='res50-ibnb-layer3', arch='resnet50',resume='', gs_save=False, combine_all=False, gs_verbose=True)\n",
        "\n",
        "main(args)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
